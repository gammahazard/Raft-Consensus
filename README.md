<p align="center">
  <img src="https://img.shields.io/badge/WASI-0.2%20Preview%202-blueviolet?style=for-the-badge&logo=webassembly" alt="WASI 0.2"/>
  <img src="https://img.shields.io/badge/Rust-1.75+-orange?style=for-the-badge&logo=rust" alt="Rust"/>
  <img src="https://img.shields.io/badge/Raft-Consensus-success?style=for-the-badge" alt="Raft"/>
  <img src="https://img.shields.io/badge/Wasmtime-Compatible-green?style=for-the-badge&logo=webassembly" alt="Wasmtime"/>
  <img src="https://img.shields.io/badge/Raspberry%20Pi-Coming%20Soon-ff69b4?style=for-the-badge&logo=raspberrypi" alt="Pi Ready"/>
</p>

<h1 align="center">ğŸ—³ï¸ Raft Consensus Cluster</h1>

<p align="center">
  <strong>"Unsinkable" Distributed Consensus via WASI 0.2<br/>
  Same binary runs in browser AND on Raspberry Pi cluster.</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/status-in_development-yellow" alt="Status"/>
  <img src="https://img.shields.io/badge/nodes-3-blue" alt="Nodes"/>
  <img src="https://img.shields.io/badge/license-MIT-blue" alt="License"/>
  <img src="https://img.shields.io/badge/mobile-responsive-blueviolet" alt="Mobile Responsive"/>
</p>

---

## ğŸ“‘ Contents

- [The Scenario](#-the-scenario-distributed-control-plane) â€” What problem this solves
- [Architecture](#ï¸-architecture) â€” How it's built
- [Why WASI + WASM?](#-why-wasi--wasm) â€” Size and portability advantages
- [Project Structure](#-project-structure) â€” File organization
- [Quick Start](#-quick-start) â€” Run it locally
- [Key-Value Store Demo](#-key-value-store-demo) â€” Interactive commands
- [Chaos Controls](#-chaos-controls) â€” Fault injection scenarios
- [Testing](#-testing) â€” Verification
- [Hardware Demo](#-hardware-demo-coming-soon) â€” Raspberry Pi cluster
- [Portfolio Context](#-portfolio-context) â€” Reliability Triad
- [License](#-license)

---

## ğŸ¯ The Scenario: Distributed Control Plane

> *"Three edge devices need to maintain consistent configuration state, even when 
> network partitions occur or individual nodes crash. Raft consensus ensures the 
> cluster agrees on every state change â€” or safely halts until quorum is restored."*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RAFT CONSENSUS CLUSTER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚         â”‚  NODE 1 â”‚â—„â”€â”€â”€â”€â–ºâ”‚  NODE 2 â”‚â—„â”€â”€â”€â”€â–ºâ”‚  NODE 3 â”‚                      â”‚
â”‚         â”‚   ğŸ”µ    â”‚      â”‚   ğŸŸ¢    â”‚      â”‚   ğŸŸ¢    â”‚                      â”‚
â”‚         â”‚ LEADER  â”‚      â”‚FOLLOWER â”‚      â”‚FOLLOWER â”‚                      â”‚
â”‚         â”‚ Log: 47 â”‚      â”‚ Log: 47 â”‚      â”‚ Log: 47 â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                             â”‚
â”‚   Demo: Kill Node 3 â†’ Cluster continues (2/3 quorum)                       â”‚
â”‚   Demo: Kill Node 2 â†’ Cluster halts (1/3 = no majority) â€” SAFETY!          â”‚
â”‚   Demo: Restart â†’ Auto-sync missed entries                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Architecture

| Component | Technology | Purpose |
|-----------|------------|---------|
| **raft-core** | Pure Rust | State machine, elections, log replication |
| **raft-storage** | Rust + std::fs | Persistence trait (IndexedDB in browser) |
| **raft-wasm** | wasm-bindgen | WASM exports for JavaScript host |
| **shim/** | JavaScript | WASI polyfills: BroadcastChannel, IndexedDB |
| **dashboard/** | Leptos + Trunk | Security console UI with chaos controls |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Browser Runtime                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Leptos Dashboard (cluster viz, KV store, event log)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  JavaScript Host Shim                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  BroadcastChannel â”‚  â”‚    IndexedDB     â”‚                     â”‚
â”‚  â”‚  (Virtual Network)â”‚  â”‚  (Virtual FS)    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WASM Nodes (x3) â€” same raft.wasm as hardware demo              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚raft-coreâ”‚ â”‚raft-    â”‚ â”‚raft-wasmâ”‚                            â”‚
â”‚  â”‚         â”‚ â”‚storage  â”‚ â”‚(exports)â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ“– **[Read full architecture doc â†’](docs/ARCHITECTURE.md)**

## ğŸ“¡ Why WASI + WASM?

| Challenge | Traditional | This Project |
|-----------|-------------|--------------|
| **OTA Updates** | 50-200 MB containers | ~500 KB WASM |
| **Satellite/offshore** | Minutes to transfer | Seconds |
| **Runtime deps** | Python/Node/Docker | Single `.wasm` file |
| **Crash isolation** | OS processes | WASM sandbox |
| **Portability** | Build per platform | Same binary everywhere |

> ğŸ›°ï¸ **For offshore rigs with 256 Kbps satellite:** WASM update takes 15 seconds vs Docker's 78 minutes.

## ğŸ“ Project Structure

```
raft-consensus/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ raft-core/          # pure raft algorithm logic
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs      # crate exports
â”‚   â”‚       â”œâ”€â”€ node.rs     # state machine, election logic
â”‚   â”‚       â”œâ”€â”€ message.rs  # rpc types: VoteRequest, AppendEntries
â”‚   â”‚       â””â”€â”€ log.rs      # replicated log management
â”‚   â”‚
â”‚   â”œâ”€â”€ raft-storage/       # persistence abstraction
â”‚   â”‚   â””â”€â”€ src/lib.rs      # Storage trait, FileStorage impl
â”‚   â”‚
â”‚   â””â”€â”€ raft-wasm/          # wasm-bindgen exports
â”‚       â””â”€â”€ src/lib.rs      # javascript-callable node lifecycle
â”‚
â”œâ”€â”€ shim/                   # javascript wasi polyfills
â”‚   â”œâ”€â”€ host.js             # WasiHost: instantiates wasm nodes
â”‚   â”œâ”€â”€ network.js          # BroadcastChannel virtual network
â”‚   â””â”€â”€ filesystem.js       # IndexedDB virtual filesystem
â”‚
â”œâ”€â”€ dashboard/              # leptos web ui
â”‚   â”œâ”€â”€ src/lib.rs          # cluster viz, kv store, event log
â”‚   â””â”€â”€ styles.css          # security console dark theme
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md     # system design rationale
    â”œâ”€â”€ RAFT_SPEC.md        # algorithm details
    â”œâ”€â”€ WASI_MAPPING.md     # std â†’ browser api mapping
    â””â”€â”€ HARDWARE_SETUP.md   # raspberry pi cluster guide
```

## ğŸš€ Quick Start

```powershell
# install trunk (build tool for leptos)
cargo install trunk

# install javascript dependencies
npm install

# run dev server with live reload
cd dashboard
trunk serve
# opens http://localhost:8080
```

**Run tests:**
```powershell
cargo test --workspace
```

## ğŸ’¾ Key-Value Store Demo

The dashboard includes an interactive key-value store to demonstrate log replication:

```
> SET temperature 72.5
âœ“ Committed (2/3 acks) @ index 48

> GET temperature
â†’ 72.5

> [Kill leader node]
> SET pressure 14.7
â³ Pending... (election in progress)
âœ“ Committed (new leader elected)
```

| State | Display |
|-------|---------|
| Pending | â³ `Pending... (awaiting quorum)` |
| Committed | âœ“ `Committed (2/3 acks) @ index 48` |
| Failed | âœ— `Failed: no quorum (only 1/3 alive)` |

## ğŸ® Chaos Controls

| Control | What Happens | Demo Value |
|---------|--------------|------------|
| **Kill Node** | Node stops responding | Basic fault tolerance |
| **Kill Leader** | Triggers new election | Leader failover |
| **Network Partition** | Isolate node(s) | Split-brain safety |
| **Slow Network** | 500ms latency | Election timeout behavior |
| **Rogue Node** | Disconnected node with high term | **PreVote protection** âœ¨ |
| **Restart All** | Recover cluster | Auto-sync verification |

### âœ¨ PreVote Protocol Demo

The "Rogue Node" demo showcases our **PreVote** implementation (Raft Thesis Section 9.6):

```
Scenario: Node 3 gets disconnected for a while
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Without PreVote:                    â”‚ With PreVote (our impl):
                                    â”‚
Node 3 keeps timing out,            â”‚ Node 3 times out but only
incrementing term to 50.            â”‚ sends "would you vote for me?"
                                    â”‚
When reconnected, Node 3's          â”‚ Other nodes reply "NO â€” we
high term forces leader to          â”‚ have a healthy leader."
STEP DOWN immediately!              â”‚
                                    â”‚ Node 3's term stays unchanged.
âŒ Cluster disrupted for no         â”‚ âœ… Cluster stays stable!
   good reason                      â”‚
```

**The "wow" moment:** Partition the cluster 1 vs 2 â€” the minority halts while majority continues. This is Byzantine fault safety in action.

## ğŸ§ª Testing

```powershell
# rust tests (raft-core logic)
cargo test --workspace

# javascript shim tests
cd shim
npm test
```

## ğŸ“ Hardware Demo (Coming Soon)

The same `raft.wasm` binary will run on a real 3-node Raspberry Pi cluster:

| Hardware | Model | Role |
|----------|-------|------|
| **Primary** | Raspberry Pi 4 (4GB) | Usually leader |
| **Follower** | Pi Zero 2 W | Node 2 |
| **Follower** | Pi Zero 2 W | Node 3 |

**Visual feedback:**
| LED Color | Meaning |
|:---------:|---------|
| ğŸ”µ Blue | Leader |
| ğŸŸ¢ Green | Follower (healthy) |
| ğŸŸ¡ Yellow | Candidate (election) |
| ğŸ”´ Red | Offline |

**What stays the same:**
- `raft.wasm` â€” identical binary, zero changes
- Raft algorithm â€” same election, replication, quorum logic
- Storage trait â€” real filesystem instead of IndexedDB

ğŸ“– **[Full hardware setup guide â†’](docs/HARDWARE_SETUP.md)**

> ğŸ¬ Demo video coming soon â€” split-screen browser + physical Pi cluster.

## ğŸ”— Portfolio Context

This project is part of a **Reliability Triad** demonstrating industrial-grade systems engineering:

| Project | Reliability Story | Mechanism |
|---------|-------------------|-----------|
| [ICS Guardian](https://github.com/gammahazard/vanguard-ics-guardian) | "I ensure the connection is safe." | Capability isolation |
| [Protocol Gateway](https://github.com/gammahazard/protocol-gateway-sandbox) | "I ensure the parser is crash-proof." | 2oo3 TMR voting |
| [**Raft Cluster**](https://github.com/gammahazard/Raft-Consensus) | "I ensure the system state is consistent." | Distributed consensus |

## ğŸŒ¿ Branch Strategy

| Branch | Purpose | Deployment |
|--------|---------|------------|
| `main` | Stable releases | Production |
| `develop` | Integration | Preview |
| `feature/*` | Feature work | â€” |

## ğŸ“œ License

MIT Â© 2026

---

<p align="center">
  <em>Built to demonstrate distributed consensus for industrial control systems.</em>
</p>
